{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and Analysis of GitHub User dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports/Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (1.8.2.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from wordcloud) (1.23.5)\n",
      "Requirement already satisfied: pillow in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from wordcloud) (9.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from wordcloud) (3.6.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from matplotlib->wordcloud) (22.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "from ast import literal_eval\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_colwidth\", 400)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Filtering/Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset\n",
    "Reading the json from the 10M GitHub Users dataset from https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/T6ZRJT.\n",
    "\n",
    "Due to the size of the dataset, it has to be loaded in chunks using the read_json() method from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_json(\"../data/data.json\", lines=True, chunksize=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Data\n",
    "Iterated through each chunk and filtered it by only accepting users that are\n",
    "1. Not suspicious\n",
    "2. Of the 'User' type\n",
    "3. Have a bio\n",
    "4. Following at least 1 other user\n",
    "5. Has at least 1 repo\n",
    "\n",
    "The filtering rules were made to be strict so that the best data can be used for providing recommendations, as there is such a large amount of data to extract from. This will also save time for calculations/adjustments made to the data later on.\n",
    "\n",
    "Only the required columns were kept in the final dataframe from this process, 'id', 'location', 'company', 'bio', 'follower_list', 'following_list' and 'repo_list'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "# Initialise list of dataframes to be kept\n",
    "chunk_list = []\n",
    "\n",
    "# Loop through all chunks\n",
    "for chunk in chunks:\n",
    "    chunk = chunk.loc[\n",
    "                        (chunk[\"following_list\"].notna())   \n",
    "                        & (chunk[\"repo_list\"].notna())\n",
    "                        & (chunk[\"is_suspicious\"] == False)\n",
    "                        & (chunk[\"type\"] == \"User\")\n",
    "                        & (chunk[\"bio\"].notna())\n",
    "                        & (chunk[\"following_list\"].map(lambda d: len(d) if d != None else None) > 0)\n",
    "                        & (chunk[\"repo_list\"].map(lambda d: len(d) if d != None else None) > 0)\n",
    "                    ]\n",
    "\n",
    "    # Only keep specified columns\n",
    "    df_filtered = chunk[[\"id\", \"location\", \"company\", \"bio\", \"follower_list\", \"following_list\", \"repo_list\"]]\n",
    "    count += 10000\n",
    "    print(count)\n",
    "    chunk_list.append(df_filtered)\n",
    "\n",
    "df = pd.concat(chunk_list)\n",
    "\n",
    "df.to_csv(\"../data/filtered_data.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altering Data Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.index:\n",
    "    repo_list = df.at[i, \"repo_list\"]\n",
    "    languages = [d[\"language\"] for d in repo_list]\n",
    "    languages = [i for i in set(languages) if i is not None]\n",
    "    df.at[i, 'repo_list'] = languages\n",
    "    print(i)\n",
    "\n",
    "df.rename(columns={\"repo_list\": \"languages\"}, inplace=True)\n",
    "df = df.loc[(df[\"languages\"].map(lambda d: len(d)) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"location\"].notna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing absent users from following_list and follower_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def existing_users(df, col):\n",
    "    found_users_list = []\n",
    "    for i in df.index:\n",
    "        following_list = df.at[i, col]\n",
    "        users_to_remove = []\n",
    "        for item in following_list:\n",
    "            if not df[df[\"id\"] == item].empty:\n",
    "                found_users_list.append(df[df[\"id\"] == item])\n",
    "            else:\n",
    "                users_to_remove.append(item)\n",
    "        df.at[i, col] = [x for x in following_list if x not in users_to_remove]\n",
    "                \n",
    "    return pd.concat(found_users_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_followed_users = existing_users(df, \"following_list\")\n",
    "# existing_following_users = existing_users(df, \"follower_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[\n",
    "    (df[\"following_list\"].map(lambda d: len(d)) > 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"languages_str\"] = df[\"languages\"].transform(lambda x: [f\"[lang_{i}]\" for i in x])\n",
    "df[\"languages_str\"] = [' '.join(map(str, l)) for l in df['languages_str']]\n",
    "\n",
    "df[\"location_str\"] = \"[loc_\" + df[\"location\"] + \"]\"\n",
    "df.loc[df[\"location_str\"].isna(), \"location_str\"] = \"\"\n",
    "\n",
    "df[\"clean_input\"] = df[\"bio\"] + \" \" + df[\"languages_str\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('job_titles.json')\n",
    "job_data = json.load(f)\n",
    "job_data = job_data[\"job-titles\"]\n",
    "\n",
    "count = 0\n",
    "\n",
    "def extract_job(row):\n",
    "    extracted_list = [str(word).lower() for word in job_data if word in str(row[\"bio\"]).lower()]\n",
    "    global count\n",
    "    count += 1\n",
    "    print(count)\n",
    "    if len(extracted_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return max(extracted_list, key=len)\n",
    "\n",
    "df['job'] = df.apply(extract_job, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lat/Long Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "\n",
    "location_notna_df = df.copy()\n",
    "location_notna_df = location_notna_df[location_notna_df[\"location\"].notna()]\n",
    "\n",
    "for i, row in location_notna_df.iterrows():\n",
    "    count += 1\n",
    "    if row[\"location\"].lower() not in locations_dict:\n",
    "        response = requests.get(f\"https://geocode.maps.co/search?q={row['location']}\")\n",
    "        if response.status_code == 200 and len(response.json()) > 0:\n",
    "            res_data = response.json()\n",
    "            df.at[i,'lat'] = res_data[0][\"lat\"]\n",
    "            df.at[i,'lon'] = res_data[0][\"lon\"]\n",
    "            locations_dict[row[\"location\"].lower()] = {\"lat\": res_data[0][\"lat\"], \"lon\": res_data[0][\"lon\"], \"new_location\": res_data[0][\"display_name\"]}\n",
    "            df.at[i,'new_location'] = res_data[0][\"display_name\"]\n",
    "        else:\n",
    "            df.at[i,'lat'] = None\n",
    "            df.at[i,'lon'] = None\n",
    "        time.sleep(0.6)\n",
    "    else:\n",
    "        print(f\"FOUND: {row['location']}\")\n",
    "        df.at[i,'lat'] = locations_dict[row[\"location\"].lower()][\"lat\"]\n",
    "        df.at[i,'lon'] = locations_dict[row[\"location\"].lower()][\"lon\"]\n",
    "        df.at[i,'new_location'] = locations_dict[row[\"location\"].lower()][\"new_location\"]\n",
    "\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/data.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsnsw\\AppData\\Local\\Temp\\ipykernel_8848\\1373787469.py:3: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df.columns = (df.columns.str.strip().str.lower()\n",
      "C:\\Users\\jsnsw\\AppData\\Local\\Temp\\ipykernel_8848\\1373787469.py:3: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df.columns = (df.columns.str.strip().str.lower()\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/data_3.csv\", delimiter=',', converters={\"follower_list\": pd.eval, \"following_list\": pd.eval, \"languages\": literal_eval})\n",
    "\n",
    "df.columns = (df.columns.str.strip().str.lower()\n",
    "              .str.replace(' ', '_')\n",
    "              .str.replace('(', '')\n",
    "              .str.replace(')', ''))\n",
    "\n",
    "df[\"follower_list\"] = df[\"follower_list\"].apply(lambda x: x.tolist())\n",
    "df[\"following_list\"] = df[\"following_list\"].apply(lambda x: x.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud\n",
    "Wordcloud generated from bio descriptions of all users. This shows that words like 'Computer Science', 'Developer', 'University' etc. are extremely common between bios which means we probably don't want to recommend users based on these keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for i in df[\"bio\"]:\n",
    "    text += str(i) + \" \"\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "x, y = np.ogrid[:300, :300]\n",
    "\n",
    "mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "mask = 255 * mask.astype(int)\n",
    "\n",
    "wc = WordCloud(\n",
    "    width=500,\n",
    "    height=500,\n",
    "    background_color=\"white\",\n",
    "    min_font_size=6,\n",
    "    repeat=True,\n",
    "    mask=mask,\n",
    ")\n",
    "wc.generate(text)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.title(f\"Most Used Words\", fontsize=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation User Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_students = len(df[df[\"job\"] == \"student\"].index)\n",
    "number_of_professionals = len(df[(df[\"job\"].notna()) & (df[\"job\"] != \"student\")].index)\n",
    "number_of_unemployed = len(df.index) - (number_of_students + number_of_professionals)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "langs = ['Student', 'Professional', 'Unemployed']\n",
    "students = [number_of_students, number_of_professionals, number_of_unemployed]\n",
    "ax.bar(langs,students)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jobs = df[\"job\"].value_counts()\n",
    "\n",
    "\n",
    "all_jobs = all_jobs[0:20]\n",
    "labels = all_jobs.index\n",
    "sizes = all_jobs.values.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "wedges, texts = ax.pie(sizes, labels=labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of User Geographic Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_plot_df = df.copy()\n",
    "map_plot_df = map_plot_df[map_plot_df[\"new_location\"].notna()]\n",
    "map_plot_df[\"country\"] = map_plot_df.apply(lambda x: x[\"new_location\"].split(\", \")[-1], axis=1)\n",
    "map_plot_df[\"follower_count\"] = map_plot_df.apply(lambda x: len(x[\"follower_list\"]), axis=1)\n",
    "map_plot_df[\"following_count\"] = map_plot_df.apply(lambda x: len(x[\"following_list\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (5.13.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from plotly) (8.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nbformat in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (5.7.3)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from nbformat) (4.17.3)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from nbformat) (5.7.1)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from nbformat) (2.16.2)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from nbformat) (5.1.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.19.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from jsonschema>=2.6->nbformat) (22.2.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from jupyter-core->nbformat) (305.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from jupyter-core->nbformat) (2.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install plotly\n",
    "%pip install nbformat \n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_geo(map_plot_df, lat=\"lat\", lon=\"lon\", hover_name=\"id\", size=map_plot_df[\"following_count\"]+1)\n",
    "fig.update_layout(title=\"World Map\", title_x=0.5)\n",
    "fig.update_traces(marker=dict(line=dict(width=0), color=\"#157D9D\"))\n",
    "fig.update_geos(\n",
    "    showcountries=True,\n",
    "    countrycolor=\"Grey\",\n",
    "    resolution=110,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer#\n",
    "# rs_df = df.copy()[[\"id\", \"following_list\"]][0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_users(df, \"following_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 5.05 GiB for an array with shape (26033, 26033) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m merged_list\n\u001b[0;32m     11\u001b[0m lat_lon_values \u001b[39m=\u001b[39m merge(df[\u001b[39m\"\u001b[39m\u001b[39mlat\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto_numpy(), df[\u001b[39m\"\u001b[39m\u001b[39mlon\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto_numpy())\n\u001b[1;32m---> 13\u001b[0m distances_matrix \u001b[39m=\u001b[39m nan_euclidean_distances(lat_lon_values, lat_lon_values)\n\u001b[0;32m     14\u001b[0m np\u001b[39m.\u001b[39mnan_to_num(distances_matrix, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m normed_dist \u001b[39m=\u001b[39m normalize(distances_matrix, norm\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39ml1\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:474\u001b[0m, in \u001b[0;36mnan_euclidean_distances\u001b[1;34m(X, Y, squared, missing_values, copy)\u001b[0m\n\u001b[0;32m    471\u001b[0m X[missing_X] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    472\u001b[0m Y[missing_Y] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 474\u001b[0m distances \u001b[39m=\u001b[39m euclidean_distances(X, Y, squared\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    476\u001b[0m \u001b[39m# Adjust distances for missing values\u001b[39;00m\n\u001b[0;32m    477\u001b[0m XX \u001b[39m=\u001b[39m X \u001b[39m*\u001b[39m X\n",
      "File \u001b[1;32mc:\\Users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:328\u001b[0m, in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[39mif\u001b[39;00m Y_norm_squared\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (\u001b[39m1\u001b[39m, Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m    323\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    324\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncompatible dimensions for Y of shape \u001b[39m\u001b[39m{\u001b[39;00mY\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    325\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mY_norm_squared of shape \u001b[39m\u001b[39m{\u001b[39;00moriginal_shape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    326\u001b[0m         )\n\u001b[1;32m--> 328\u001b[0m \u001b[39mreturn\u001b[39;00m _euclidean_distances(X, Y, X_norm_squared, Y_norm_squared, squared)\n",
      "File \u001b[1;32mc:\\Users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:369\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[1;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[0;32m    366\u001b[0m     distances \u001b[39m=\u001b[39m _euclidean_distances_upcast(X, XX, Y, YY)\n\u001b[0;32m    367\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    368\u001b[0m     \u001b[39m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     distances \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m safe_sparse_dot(X, Y\u001b[39m.\u001b[39;49mT, dense_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    370\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m XX\n\u001b[0;32m    371\u001b[0m     distances \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m YY\n",
      "File \u001b[1;32mc:\\Users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages\\sklearn\\utils\\extmath.py:189\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    187\u001b[0m         ret \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(a, b)\n\u001b[0;32m    188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m     ret \u001b[39m=\u001b[39m a \u001b[39m@\u001b[39;49m b\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    192\u001b[0m     sparse\u001b[39m.\u001b[39missparse(a)\n\u001b[0;32m    193\u001b[0m     \u001b[39mand\u001b[39;00m sparse\u001b[39m.\u001b[39missparse(b)\n\u001b[0;32m    194\u001b[0m     \u001b[39mand\u001b[39;00m dense_output\n\u001b[0;32m    195\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(ret, \u001b[39m\"\u001b[39m\u001b[39mtoarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    196\u001b[0m ):\n\u001b[0;32m    197\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39mtoarray()\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 5.05 GiB for an array with shape (26033, 26033) and data type float64"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import nan_euclidean_distances\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def merge(list1, list2):  \n",
    "    # listnp.nan_to_num(list1, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "    # np.nan_to_num(list2, copy=True, nan=0.0, posinf=None, neginf=None)\n",
    "    merged_list = [(list1[i], list2[i]) for i in range(0, len(list1))]\n",
    "    return merged_list\n",
    "\n",
    "lat_lon_values = merge(df[\"lat\"].to_numpy(), df[\"lon\"].to_numpy())\n",
    "distances_matrix = nan_euclidean_distances(lat_lon_values, lat_lon_values)\n",
    "np.nan_to_num(distances_matrix, copy=False)\n",
    "normed_dist = normalize(distances_matrix, norm=\"l1\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(strip_accents=\"unicode\", stop_words=\"english\", min_df=3)\n",
    "vecs = vec.fit_transform(df[\"clean_input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_by_content(sim):\n",
    "    scores = enumerate(sim)\n",
    "    sorted_scores=sorted(scores,key=lambda x:x[1], reverse=True)\n",
    "    return sorted_scores[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11415, array([0.23369523])),\n",
       " (23452, array([0.23003008])),\n",
       " (17489, array([0.22491517])),\n",
       " (4245, array([0.22305085])),\n",
       " (12414, array([0.21943793]))]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using User 2000\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# def normalize(arr, t_min, t_max):\n",
    "#     norm_arr = []\n",
    "#     diff = t_max - t_min\n",
    "#     diff_arr = max(arr) - min(arr)   \n",
    "#     for i in arr:\n",
    "#         temp = (((i - min(arr))*diff)/diff_arr) + t_min\n",
    "#         norm_arr.append(temp)\n",
    "#     return norm_arr\n",
    "\n",
    "# 20% weight for each user\n",
    "def top_5_similarities(following_list, weight=1):\n",
    "    arr_list = []\n",
    "    for user in following_list:\n",
    "        sim = cosine_similarity(vecs, vecs[user])\n",
    "        sim[user] = 0\n",
    "        weighted_sim = (1 * (sim*weight)) # + (0.25 * normed_dist[user])\n",
    "        arr_list.append(weighted_sim)\n",
    "    return sum(arr_list)\n",
    "\n",
    "\n",
    "def get_recent_follows(user):\n",
    "    following = df.loc[user].following_list\n",
    "    return [df[df[\"id\"] == i].index[0] for i in following if not df[df[\"id\"] == i].empty]\n",
    "\n",
    "# user_sim = top_5_similarities(get_recent_follows(16000), 0.2)\n",
    "\n",
    "# similarity_by_content(user_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                    26869558\n",
       "location                                                       Berlin, Germany\n",
       "company                                                                    NaN\n",
       "bio                                            Full Stack JavaScript Developer\n",
       "follower_list      [12866783, 26869552, 17891156, 12187795, 6364656, 29039300]\n",
       "following_list                            [11701, 6364656, 12187795, 12866783]\n",
       "languages                                                    [JavaScript, CSS]\n",
       "languages_str                                     [lang_JavaScript] [lang_CSS]\n",
       "location_str                                             [loc_Berlin, Germany]\n",
       "clean_input       Full Stack JavaScript Developer [lang_JavaScript] [lang_CSS]\n",
       "lat                                                                  52.517037\n",
       "lon                                                                   13.38886\n",
       "new_location                                               Berlin, Deutschland\n",
       "job                                                       javascript developer\n",
       "Name: 3478, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[3478]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15944, 12014, 7422, 12475, 15616, 6047, 7605, 3403, 1588, 3702]\n"
     ]
    }
   ],
   "source": [
    "# df.iloc[2000].following_list\n",
    "df.iloc[6000].following_list\n",
    "\n",
    "def get_idx_by_id(user):\n",
    "    following_list = df.iloc[user].following_list\n",
    "    return [df[df[\"id\"] == i].index[0] for i in following_list]\n",
    "\n",
    "print(get_idx_by_id(6000))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_df_exploded = rs_df[\"following_list\"].explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsnsw\\AppData\\Local\\Temp\\ipykernel_35756\\3199409533.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  cf_df = pd.get_dummies(rs_df_exploded).groupby(level=0).sum()\n"
     ]
    }
   ],
   "source": [
    "cf_df = pd.get_dummies(rs_df_exploded).groupby(level=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "# Create a MultiLabelBinarizer object\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit the object on the following column and transform it into a sparse matrix\n",
    "sparse_matrix = mlb.fit_transform(df['following_list'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 1, 1],\n",
       "       [0, 0, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame({\"id\": [1, 2, 3, 4, 5], \"following_list\": [[3, 2], [], [1], [5, 3, 1], []]})\n",
    "sparse_matrix = mlb.fit_transform(test_df['following_list'])\n",
    "\n",
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "id_dict = dict(zip(df['id'], range(len(df))))\n",
    "\n",
    "data = []\n",
    "for i, row in df.iterrows():\n",
    "    user_id = row['id']\n",
    "    following = row['following_list']\n",
    "    row_index = id_dict[user_id]\n",
    "    for f in following:\n",
    "        if f in id_dict:\n",
    "            col_index = id_dict[f]\n",
    "            data.append((row_index, col_index, 1))\n",
    "\n",
    "n_users = len(df)\n",
    "n_items = len(df)\n",
    "sparse_matrix = csr_matrix((np.ones(len(data), dtype=np.int32), (np.array([x[0] for x in data]), np.array([x[1] for x in data]))), shape=(n_users, n_items), dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_following(userId):\n",
    "    profile_cs = cosine_similarity(sparse_matrix, sparse_matrix[userId])\n",
    "    scores = enumerate(profile_cs)\n",
    "    sorted_scores=sorted(scores,key=lambda x:x[1], reverse=True)\n",
    "    sorted_scores=[i for i in sorted_scores if i[0] != userId]\n",
    "    return sorted_scores[0:5]\n",
    "\n",
    "# recommend_by_following(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(6871, array([0.40824829])),\n",
       " (0, array([0.])),\n",
       " (1, array([0.])),\n",
       " (2, array([0.])),\n",
       " (3, array([0.]))]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_by_following(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(419, array([0.51639778])), (9232, array([0.4472136])), (9328, array([0.4472136])), (11942, array([0.4472136])), (15088, array([0.4472136]))]\n",
      "[(19791, array([0.40097003])), (11890, array([0.35867227])), (759, array([0.34375507])), (2509, array([0.34199371])), (21407, array([0.33658565]))]\n"
     ]
    }
   ],
   "source": [
    "def recommend(user):\n",
    "    user_sim = top_5_similarities(get_recent_follows(user), 0.2)\n",
    "    collaborative_f = recommend_by_following(user)\n",
    "    content_f = similarity_by_content(user_sim)\n",
    "    print(collaborative_f)\n",
    "    print(content_f)\n",
    "    [i for i in ]\n",
    "recommend(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                                                                                                    22639736\n",
       "location                                                                                                                                               Granada\n",
       "company                                                                                                                                                    NaN\n",
       "bio                                                                                                                  Mathematics and Computer Engineering. UGR\n",
       "follower_list     [13903165, 17575711, 23178033, 22728603, 15173583, 11518373, 14114187, 22918075, 24752765, 22836496, 32845717, 23448652, 11302859, 22639789]\n",
       "following_list                                                                                                                            [11518373, 13903165]\n",
       "languages                                                                                                                                     [Java, TeX, C++]\n",
       "languages_str                                                                                                                [lang_Java] [lang_TeX] [lang_C++]\n",
       "location_str                                                                                                                                     [loc_Granada]\n",
       "clean_input                                                          Mathematics and Computer Engineering. UGR [lang_Java] [lang_TeX] [lang_C++] [loc_Granada]\n",
       "lat                                                                                                                                                  37.173499\n",
       "lon                                                                                                                                                  -3.599534\n",
       "new_location                                                                                Granada, Comarca de la Vega de Granada, Granada, Andalucía, España\n",
       "job                                                                                                                                          computer engineer\n",
       "Name: 21407, dtype: object"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[21407]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_recommendations(n, user_id):\n",
    "    rec_list = []\n",
    "    top_10_list = recommend(user_id)\n",
    "    top_10_list_ids = [i[0] for i in top_10_list]\n",
    "    for i in top_10_list:\n",
    "        rec_list.append(df.iloc[i[0]])\n",
    "    top_10_df = pd.concat(rec_list, axis=1).transpose()\n",
    "    top_10_df[\"similarity\"] = [i[1] for i in top_10_list]\n",
    "    top_10_df = top_10_df[[\"following_list\", \"similarity\"]]\n",
    "    return top_10_df\n",
    "\n",
    "    \n",
    "top_10_df = get_top_10(909, sim)\n",
    "top_10_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a new user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_bio(df, text):\n",
    "\n",
    "    queryTFIDF = vec.fit_transform(df[\"clean_input\"].apply(lambda x: np.str_(x)))\n",
    "    df.loc[len(df)] = [9999, 9999, 5462462, \"Edinburgh\", \"CodeClan\", \"I like JavaScript\", [], [], [\"HTML\", \"CSS\", \"JavaScript\"], \"HTML CSS JavaScript\", \"I like JavaScript HTML CSS JavaScript\"]\n",
    "    new_data = df.iloc[len(df)-1]\n",
    "    queryTFIDF_2 = vec.transform([new_data[\"clean_input\"]])\n",
    "    cosine_similarities = cosine_similarity(queryTFIDF, queryTFIDF_2).flatten()\n",
    "    # # print(cosine_similarities)\n",
    "    return cosine_similarities\n",
    "# new_sim = add_bio(professionals_df, \"Hello I like web development\")\n",
    "# print(len(new_sim))\n",
    "\n",
    "new_sim_con = np.vstack((cos_sim, new_sim))\n",
    "new_sim = np.append(new_sim, 1)\n",
    "\n",
    "\n",
    "# new_sim = new_sim.reshape(-1, 1)\n",
    "# print(new_sim)\n",
    "# sim = np.concatenate((new_sim_con, new_sim), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_follows_df = df.copy()\n",
    "\n",
    "dum = pd.get_dummies(high_follows_df['languages'].explode()).sum(level=0)\n",
    "dum_sim = cosine_similarity(dum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_languages(userId):\n",
    "    print(\"Languages of recommendee:\")\n",
    "    print(high_follows_df.iloc[userId].languages)\n",
    "    scores = list(enumerate(dum_sim[userId]))\n",
    "    sorted_scores=sorted(scores,key=lambda x:x[1], reverse=True)\n",
    "    sorted_scores=sorted_scores[1:]\n",
    "    recommendations = [user for user in sorted_scores]\n",
    "    return recommendations\n",
    "\n",
    "def get_top_10(rec):\n",
    "    rec_list = []\n",
    "    top_10_list = rec[0:10]\n",
    "    top_10_list_ids = [i[0] for i in top_10_list]\n",
    "    for i in top_10_list:\n",
    "        rec_list.append(high_follows_df.iloc[i[0]])\n",
    "    top_10_df = pd.concat(rec_list, axis=1).transpose()\n",
    "    top_10_df[\"similarity\"] = [i[1] for i in top_10_list]\n",
    "    top_10_df = top_10_df[[\"languages\", \"similarity\"]]\n",
    "    return top_10_df\n",
    "\n",
    "rec = recommend_by_languages(10000)\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Recommendations\")\n",
    "print(get_top_10(rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_follows_df[\"follower_count\"] = high_follows_df[\"follower_list\"].str.len()\n",
    "high_follows_df = high_follows_df.sort_values(by='follower_count', ascending=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connections Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import math\n",
    "\n",
    "\n",
    "graph = nx.DiGraph()\n",
    "sub_graph = high_follows_df\n",
    "\n",
    "for index, row in sub_graph.iterrows():\n",
    "    graph.add_node(row[\"id\"])\n",
    "\n",
    "for index, row in sub_graph.iterrows():\n",
    "    f_list = row[\"follower_list\"]\n",
    "    for follower in f_list:\n",
    "        graph.add_edge(follower, row[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab\n",
    "\n",
    "\n",
    "\n",
    "def save_graph(graph,file_name):\n",
    "    #initialze Figure\n",
    "    plt.figure(num=None, figsize=(150, 150), dpi=100)\n",
    "    plt.axis('off')\n",
    "    fig = plt.figure(1)\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw_networkx_nodes(graph,pos,node_color=range(len(graph)),cmap=plt.cm.Reds)\n",
    "    nx.draw_networkx_edges(graph,pos,alpha=0.4,arrows=False)\n",
    "\n",
    "    # cut = 1.00\n",
    "    # xmax = cut * max(xx for xx, yy in pos.values())\n",
    "    # ymax = cut * max(yy for xx, yy in pos.values())\n",
    "    # plt.xlim(0, xmax)\n",
    "    # plt.ylim(0, ymax)\n",
    "\n",
    "    plt.savefig(file_name)\n",
    "    pylab.close()\n",
    "    del fig\n",
    "\n",
    "#Assuming that the graph g has nodes and edges entered\n",
    "print(len(graph))\n",
    "save_graph(graph,\"my_graph_7.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2b37dc4820977dd01f97017c0afe691699a3f985416c7f071ed6150a7a27c17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
