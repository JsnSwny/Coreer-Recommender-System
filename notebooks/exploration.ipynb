{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and Analysis of GitHub User dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports/Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (1.8.2.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from wordcloud) (1.23.5)\n",
      "Requirement already satisfied: pillow in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from wordcloud) (9.3.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from wordcloud) (3.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from matplotlib->wordcloud) (22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "from ast import literal_eval\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"max_colwidth\", 400)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Filtering/Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset\n",
    "Reading the json from the 10M GitHub Users dataset from https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/T6ZRJT.\n",
    "\n",
    "Due to the size of the dataset, it has to be loaded in chunks using the read_json() method from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_json(\"../data/data.json\", lines=True, chunksize=10000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Data\n",
    "Iterated through each chunk and filtered it by only accepting users that are\n",
    "1. Not suspicious\n",
    "2. Of the 'User' type\n",
    "3. Have a bio\n",
    "4. Following at least 1 other user\n",
    "5. Has at least 1 repo\n",
    "\n",
    "The filtering rules were made to be strict so that the best data can be used for providing recommendations, as there is such a large amount of data to extract from. This will also save time for calculations/adjustments made to the data later on.\n",
    "\n",
    "Only the required columns were kept in the final dataframe from this process, 'id', 'location', 'company', 'bio', 'follower_list', 'following_list' and 'repo_list'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "# Initialise list of dataframes to be kept\n",
    "chunk_list = []\n",
    "\n",
    "# Loop through all chunks\n",
    "for chunk in chunks:\n",
    "    chunk = chunk.loc[\n",
    "                        (chunk[\"following_list\"].notna())   \n",
    "                        & (chunk[\"repo_list\"].notna())\n",
    "                        & (chunk[\"is_suspicious\"] == False)\n",
    "                        & (chunk[\"type\"] == \"User\")\n",
    "                        & (chunk[\"bio\"].notna())\n",
    "                        & (chunk[\"following_list\"].map(lambda d: len(d) if d != None else None) > 0)\n",
    "                        & (chunk[\"repo_list\"].map(lambda d: len(d) if d != None else None) > 0)\n",
    "                    ]\n",
    "\n",
    "    # Only keep specified columns\n",
    "    df_filtered = chunk[[\"id\", \"location\", \"company\", \"bio\", \"follower_list\", \"following_list\", \"repo_list\"]]\n",
    "    count += 10000\n",
    "    print(count)\n",
    "    chunk_list.append(df_filtered)\n",
    "\n",
    "df = pd.concat(chunk_list)\n",
    "\n",
    "df.to_csv(\"../data/filtered_data.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altering Data Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.index:\n",
    "    repo_list = df.at[i, \"repo_list\"]\n",
    "    languages = [d[\"language\"] for d in repo_list]\n",
    "    languages = [i for i in set(languages) if i is not None]\n",
    "    df.at[i, 'repo_list'] = languages\n",
    "    print(i)\n",
    "\n",
    "df.rename(columns={\"repo_list\": \"languages\"}, inplace=True)\n",
    "df = df.loc[(df[\"languages\"].map(lambda d: len(d)) > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"location\"].notna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing absent users from following_list and follower_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def existing_users(df, col):\n",
    "    found_users_list = []\n",
    "    for i in df.index:\n",
    "        following_list = df.at[i, col]\n",
    "        users_to_remove = []\n",
    "        for item in following_list:\n",
    "            if not df[df[\"id\"] == item].empty:\n",
    "                found_users_list.append(df[df[\"id\"] == item])\n",
    "            else:\n",
    "                users_to_remove.append(item)\n",
    "        df.at[i, col] = [x for x in following_list if x not in users_to_remove]\n",
    "                \n",
    "    return pd.concat(found_users_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_followed_users = existing_users(df, \"following_list\")\n",
    "# existing_following_users = existing_users(df, \"follower_list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[\n",
    "    (df[\"following_list\"].map(lambda d: len(d)) > 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"languages_str\"] = df[\"languages\"].transform(lambda x: [f\"[lang_{i}]\" for i in x])\n",
    "df[\"languages_str\"] = [' '.join(map(str, l)) for l in df['languages_str']]\n",
    "\n",
    "df[\"location_str\"] = \"[loc_\" + df[\"location\"] + \"]\"\n",
    "df.loc[df[\"location_str\"].isna(), \"location_str\"] = \"\"\n",
    "\n",
    "df[\"clean_input\"] = df[\"bio\"] + \" \" + df[\"languages_str\"] + \" \" + df[\"location_str\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Job Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('job_titles.json')\n",
    "job_data = json.load(f)\n",
    "job_data = job_data[\"job-titles\"]\n",
    "\n",
    "count = 0\n",
    "\n",
    "def extract_job(row):\n",
    "    extracted_list = [str(word).lower() for word in job_data if word in str(row[\"bio\"]).lower()]\n",
    "    global count\n",
    "    count += 1\n",
    "    print(count)\n",
    "    if len(extracted_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return max(extracted_list, key=len)\n",
    "\n",
    "df['job'] = df.apply(extract_job, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lat/Long Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "\n",
    "\n",
    "location_notna_df = df.copy()\n",
    "location_notna_df = location_notna_df[location_notna_df[\"location\"].notna()]\n",
    "\n",
    "for i, row in location_notna_df.iterrows():\n",
    "    count += 1\n",
    "    if row[\"location\"].lower() not in locations_dict:\n",
    "        response = requests.get(f\"https://geocode.maps.co/search?q={row['location']}\")\n",
    "        if response.status_code == 200 and len(response.json()) > 0:\n",
    "            res_data = response.json()\n",
    "            df.at[i,'lat'] = res_data[0][\"lat\"]\n",
    "            df.at[i,'lon'] = res_data[0][\"lon\"]\n",
    "            locations_dict[row[\"location\"].lower()] = {\"lat\": res_data[0][\"lat\"], \"lon\": res_data[0][\"lon\"], \"new_location\": res_data[0][\"display_name\"]}\n",
    "            df.at[i,'new_location'] = res_data[0][\"display_name\"]\n",
    "        else:\n",
    "            df.at[i,'lat'] = None\n",
    "            df.at[i,'lon'] = None\n",
    "        time.sleep(0.6)\n",
    "    else:\n",
    "        print(f\"FOUND: {row['location']}\")\n",
    "        df.at[i,'lat'] = locations_dict[row[\"location\"].lower()][\"lat\"]\n",
    "        df.at[i,'lon'] = locations_dict[row[\"location\"].lower()][\"lon\"]\n",
    "        df.at[i,'new_location'] = locations_dict[row[\"location\"].lower()][\"new_location\"]\n",
    "\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/data.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsnsw\\AppData\\Local\\Temp\\ipykernel_12884\\1373787469.py:3: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df.columns = (df.columns.str.strip().str.lower()\n",
      "C:\\Users\\jsnsw\\AppData\\Local\\Temp\\ipykernel_12884\\1373787469.py:3: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df.columns = (df.columns.str.strip().str.lower()\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/data.csv\", delimiter=',', converters={\"follower_list\": pd.eval, \"following_list\": pd.eval, \"languages\": literal_eval})\n",
    "\n",
    "df.columns = (df.columns.str.strip().str.lower()\n",
    "              .str.replace(' ', '_')\n",
    "              .str.replace('(', '')\n",
    "              .str.replace(')', ''))\n",
    "\n",
    "df[\"follower_list\"] = df[\"follower_list\"].apply(lambda x: x.tolist())\n",
    "df[\"following_list\"] = df[\"following_list\"].apply(lambda x: x.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud\n",
    "Wordcloud generated from bio descriptions of all users. This shows that words like 'Computer Science', 'Developer', 'University' etc. are extremely common between bios which means we probably don't want to recommend users based on these keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "for i in df[\"bio\"]:\n",
    "    text += str(i) + \" \"\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "x, y = np.ogrid[:300, :300]\n",
    "\n",
    "mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "mask = 255 * mask.astype(int)\n",
    "\n",
    "wc = WordCloud(\n",
    "    width=500,\n",
    "    height=500,\n",
    "    background_color=\"white\",\n",
    "    min_font_size=6,\n",
    "    repeat=True,\n",
    "    mask=mask,\n",
    ")\n",
    "wc.generate(text)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.title(f\"Most Used Words\", fontsize=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation User Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_students = len(df[df[\"job\"] == \"student\"].index)\n",
    "number_of_professionals = len(df[(df[\"job\"].notna()) & (df[\"job\"] != \"student\")].index)\n",
    "number_of_unemployed = len(df.index) - (number_of_students + number_of_professionals)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "langs = ['Student', 'Professional', 'Unemployed']\n",
    "students = [number_of_students, number_of_professionals, number_of_unemployed]\n",
    "ax.bar(langs,students)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jobs = df[\"job\"].value_counts()\n",
    "\n",
    "\n",
    "all_jobs = all_jobs[0:20]\n",
    "labels = all_jobs.index\n",
    "sizes = all_jobs.values.tolist()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "wedges, texts = ax.pie(sizes, labels=labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of User Geographic Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_plot_df = df.copy()\n",
    "map_plot_df = map_plot_df[map_plot_df[\"new_location\"].notna()]\n",
    "map_plot_df[\"country\"] = map_plot_df.apply(lambda x: x[\"new_location\"].split(\", \")[-1], axis=1)\n",
    "map_plot_df[\"follower_count\"] = map_plot_df.apply(lambda x: len(x[\"follower_list\"]), axis=1)\n",
    "map_plot_df[\"following_count\"] = map_plot_df.apply(lambda x: len(x[\"following_list\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (5.13.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from plotly) (8.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nbformat in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (5.7.3)\n",
      "Requirement already satisfied: jsonschema>=2.6 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from nbformat) (4.17.3)\n",
      "Requirement already satisfied: traitlets>=5.1 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from nbformat) (5.7.1)\n",
      "Requirement already satisfied: fastjsonschema in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from nbformat) (2.16.2)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from nbformat) (5.1.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from jsonschema>=2.6->nbformat) (0.19.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from jsonschema>=2.6->nbformat) (22.2.0)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from jupyter-core->nbformat) (305.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\jsnsw\\anaconda3\\envs\\recsys\\lib\\site-packages (from jupyter-core->nbformat) (2.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install plotly\n",
    "%pip install nbformat \n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_geo(map_plot_df, lat=\"lat\", lon=\"lon\", hover_name=\"id\", size=map_plot_df[\"following_count\"]+1)\n",
    "fig.update_layout(title=\"World Map\", title_x=0.5)\n",
    "fig.update_traces(marker=dict(line=dict(width=0), color=\"#157D9D\"))\n",
    "fig.update_geos(\n",
    "    showcountries=True,\n",
    "    countrycolor=\"Grey\",\n",
    "    resolution=110,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer#\n",
    "# rs_df = df.copy()[[\"id\", \"following_list\"]][0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[0:20000]\n",
    "existing_users(df, \"following_list\")\n",
    "df = df.loc[\n",
    "    (df[\"following_list\"].map(lambda d: len(d)) > 0)\n",
    "]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content-based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(strip_accents=\"unicode\", stop_words=\"english\", min_df=3)\n",
    "vecs = vec.fit_transform(df[\"clean_input\"].apply(lambda x: np.str_(x)))\n",
    "sim = cosine_similarity(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recommend_old(userId, sim_mat):\n",
    "#     print(\"Recommendee:\")\n",
    "#     print(df.iloc[userId])\n",
    "#     scores = list(enumerate(sim_mat[userId]))\n",
    "#     sorted_scores=sorted(scores,key=lambda x:x[1], reverse=True)\n",
    "#     sorted_scores=sorted_scores[1:]\n",
    "#     recommendations = [user for user in sorted_scores]\n",
    "#     return recommendations\n",
    "\n",
    "def similarity_by_content(sim):\n",
    "    scores = enumerate(sim)\n",
    "    sorted_scores=sorted(scores,key=lambda x:x[1], reverse=True)\n",
    "    return sorted_scores[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4995, 0.7295590184807418),\n",
       " (13022, 0.7175073082570949),\n",
       " (8518, 0.7090515214622456),\n",
       " (7524, 0.6759967385804102),\n",
       " (14075, 0.6757850726401022)]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using User 2000\n",
    "\n",
    "# 20% weight for each user\n",
    "def top_5_similarities(following_list, weight=1):\n",
    "    arr_list = []\n",
    "    for user in following_list:\n",
    "        sim[user][user] = 0\n",
    "        weighted_sim = sim[user]*weight\n",
    "        arr_list.append(weighted_sim)\n",
    "        \n",
    "    return sum(arr_list)\n",
    "\n",
    "\n",
    "def get_recent_follows(user):\n",
    "    following = df.loc[user].following_list\n",
    "    return [df[df[\"id\"] == i].index[0] for i in following]\n",
    "\n",
    "user_sim = top_5_similarities(get_recent_follows(6000), 0.2)\n",
    "\n",
    "similarity_by_content(user_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15944, 12014, 7422, 12475, 15616, 6047, 7605, 3403, 1588, 3702]\n"
     ]
    }
   ],
   "source": [
    "# df.iloc[2000].following_list\n",
    "df.iloc[6000].following_list\n",
    "\n",
    "def get_idx_by_id(user):\n",
    "    following_list = df.iloc[user].following_list\n",
    "    return [df[df[\"id\"] == i].index[0] for i in following_list]\n",
    "\n",
    "print(get_idx_by_id(6000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                                                                                                                              26792182\n",
       "location                                                                                                                                                              São Paulo , Brasil\n",
       "company                                                                                                                                                                   @minds4health \n",
       "bio                                                                                                                                                                FULL STACK  DEVELOPER\n",
       "follower_list                                                                                                                                             [21126731, 20522931, 40696068]\n",
       "following_list                                                                                                                                                                 [2530054]\n",
       "languages                                                                                                                                             [CSS, Java, PHP, JavaScript, HTML]\n",
       "languages_str                                                                                                            [lang_CSS] [lang_Java] [lang_PHP] [lang_JavaScript] [lang_HTML]\n",
       "location_str                                                                                                                                                    [loc_São Paulo , Brasil]\n",
       "clean_input                                                               FULL STACK  DEVELOPER [lang_CSS] [lang_Java] [lang_PHP] [lang_JavaScript] [lang_HTML] [loc_São Paulo , Brasil]\n",
       "lat                                                                                                                                                                           -23.550651\n",
       "lon                                                                                                                                                                           -46.633382\n",
       "new_location      São Paulo, Região Imediata de São Paulo, Região Metropolitana de São Paulo, Região Geográfica Intermediária de São Paulo, São Paulo, Região Sudeste, 01001-000, Brasil\n",
       "job                                                                                                                                                                                  NaN\n",
       "Name: 14075, dtype: object"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[14075]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_df_exploded = rs_df[\"following_list\"].explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jsnsw\\AppData\\Local\\Temp\\ipykernel_35756\\3199409533.py:1: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  cf_df = pd.get_dummies(rs_df_exploded).groupby(level=0).sum()\n"
     ]
    }
   ],
   "source": [
    "cf_df = pd.get_dummies(rs_df_exploded).groupby(level=0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28596\n"
     ]
    }
   ],
   "source": [
    "profile_sim_score_matrix = np.zeros((len(df.index), len(df.index)))\n",
    "profile_sim_score_matrix\n",
    "\n",
    "count = 0\n",
    "for x in range(len(df.index)):\n",
    "    for y in df.iloc[x].following_list:\n",
    "        match = df[df[\"id\"] == y]\n",
    "        if not match.empty:\n",
    "            profile_sim_score_matrix[x, match.index] = 1\n",
    "            count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_cs = cosine_similarity(profile_sim_score_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_mat = cf_df.to_numpy()\n",
    "cf_cs = cosine_similarity(cf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_following(userId):\n",
    "    scores = list(enumerate(profile_cs[userId]))\n",
    "    sorted_scores=sorted(scores,key=lambda x:x[1], reverse=True)\n",
    "    sorted_scores=[i for i in sorted_scores if i[0] != userId]\n",
    "    return sorted_scores[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7605, 0.4743416490252569),\n",
       " (15616, 0.3651483716701108),\n",
       " (1243, 0.31622776601683794),\n",
       " (1591, 0.31622776601683794),\n",
       " (4095, 0.31622776601683794),\n",
       " (5739, 0.31622776601683794),\n",
       " (8188, 0.31622776601683794),\n",
       " (9140, 0.31622776601683794),\n",
       " (11646, 0.31622776601683794),\n",
       " (14223, 0.31622776601683794)]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_by_following(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7605, 0.4743416490252569), (15616, 0.3651483716701108), (1243, 0.31622776601683794), (1591, 0.31622776601683794), (4095, 0.31622776601683794)]\n",
      "[(4995, 0.7295590184807418), (13022, 0.7175073082570949), (8518, 0.7090515214622456), (7524, 0.6759967385804102), (14075, 0.6757850726401022)]\n"
     ]
    }
   ],
   "source": [
    "def recommend(user):\n",
    "    user_sim = top_5_similarities(get_recent_follows(user), 0.2)\n",
    "    collaborative_f = recommend_by_following(user)\n",
    "    content_f = similarity_by_content(user_sim)\n",
    "    print(collaborative_f)\n",
    "    print(content_f)\n",
    "\n",
    "recommend(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                                                                                                                               3841796\n",
       "location                                                                                                                                                              São Paulo / Brasil\n",
       "company                                                                                                                                                                              IBM\n",
       "bio                                                                                                                                                                        Web Developer\n",
       "follower_list                                                                                                             [5922108, 25665, 1517707, 966337, 7927942, 10522957, 35695600]\n",
       "following_list                                                                                                                                                                 [5922108]\n",
       "languages                                                                                                                                                  [PHP, Ruby, JavaScript, HTML]\n",
       "languages_str                                                                                                                       [lang_PHP] [lang_Ruby] [lang_JavaScript] [lang_HTML]\n",
       "location_str                                                                                                                                                    [loc_São Paulo / Brasil]\n",
       "clean_input                                                                                  Web Developer [lang_PHP] [lang_Ruby] [lang_JavaScript] [lang_HTML] [loc_São Paulo / Brasil]\n",
       "lat                                                                                                                                                                           -23.550651\n",
       "lon                                                                                                                                                                           -46.633382\n",
       "new_location      São Paulo, Região Imediata de São Paulo, Região Metropolitana de São Paulo, Região Geográfica Intermediária de São Paulo, São Paulo, Região Sudeste, 01001-000, Brasil\n",
       "job                                                                                                                                                                        web developer\n",
       "Name: 7524, dtype: object"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[7524]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10(user_id, sim_mat):\n",
    "    rec_list = []\n",
    "    top_10_list = recommend(user_id, sim_mat)[0:20]\n",
    "    top_10_list_ids = [i[0] for i in top_10_list]\n",
    "    for i in top_10_list:\n",
    "        rec_list.append(df.iloc[i[0]])\n",
    "    top_10_df = pd.concat(rec_list, axis=1).transpose()\n",
    "    top_10_df[\"similarity\"] = [i[1] for i in top_10_list]\n",
    "    top_10_df = top_10_df[[\"following_list\", \"similarity\"]]\n",
    "    return top_10_df\n",
    "\n",
    "    \n",
    "top_10_df = get_top_10(909, sim)\n",
    "top_10_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a new user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_bio(df, text):\n",
    "\n",
    "    queryTFIDF = vec.fit_transform(df[\"clean_input\"].apply(lambda x: np.str_(x)))\n",
    "    df.loc[len(df)] = [9999, 9999, 5462462, \"Edinburgh\", \"CodeClan\", \"I like JavaScript\", [], [], [\"HTML\", \"CSS\", \"JavaScript\"], \"HTML CSS JavaScript\", \"I like JavaScript HTML CSS JavaScript\"]\n",
    "    new_data = df.iloc[len(df)-1]\n",
    "    queryTFIDF_2 = vec.transform([new_data[\"clean_input\"]])\n",
    "    cosine_similarities = cosine_similarity(queryTFIDF, queryTFIDF_2).flatten()\n",
    "    # # print(cosine_similarities)\n",
    "    return cosine_similarities\n",
    "# new_sim = add_bio(professionals_df, \"Hello I like web development\")\n",
    "# print(len(new_sim))\n",
    "\n",
    "new_sim_con = np.vstack((cos_sim, new_sim))\n",
    "new_sim = np.append(new_sim, 1)\n",
    "\n",
    "\n",
    "# new_sim = new_sim.reshape(-1, 1)\n",
    "# print(new_sim)\n",
    "# sim = np.concatenate((new_sim_con, new_sim), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_follows_df = df.copy()\n",
    "\n",
    "dum = pd.get_dummies(high_follows_df['languages'].explode()).sum(level=0)\n",
    "dum_sim = cosine_similarity(dum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_by_languages(userId):\n",
    "    print(\"Languages of recommendee:\")\n",
    "    print(high_follows_df.iloc[userId].languages)\n",
    "    scores = list(enumerate(dum_sim[userId]))\n",
    "    sorted_scores=sorted(scores,key=lambda x:x[1], reverse=True)\n",
    "    sorted_scores=sorted_scores[1:]\n",
    "    recommendations = [user for user in sorted_scores]\n",
    "    return recommendations\n",
    "\n",
    "def get_top_10(rec):\n",
    "    rec_list = []\n",
    "    top_10_list = rec[0:10]\n",
    "    top_10_list_ids = [i[0] for i in top_10_list]\n",
    "    for i in top_10_list:\n",
    "        rec_list.append(high_follows_df.iloc[i[0]])\n",
    "    top_10_df = pd.concat(rec_list, axis=1).transpose()\n",
    "    top_10_df[\"similarity\"] = [i[1] for i in top_10_list]\n",
    "    top_10_df = top_10_df[[\"languages\", \"similarity\"]]\n",
    "    return top_10_df\n",
    "\n",
    "rec = recommend_by_languages(10000)\n",
    "\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Recommendations\")\n",
    "print(get_top_10(rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_follows_df[\"follower_count\"] = high_follows_df[\"follower_list\"].str.len()\n",
    "high_follows_df = high_follows_df.sort_values(by='follower_count', ascending=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connections Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import math\n",
    "\n",
    "\n",
    "graph = nx.DiGraph()\n",
    "sub_graph = high_follows_df\n",
    "\n",
    "for index, row in sub_graph.iterrows():\n",
    "    graph.add_node(row[\"id\"])\n",
    "\n",
    "for index, row in sub_graph.iterrows():\n",
    "    f_list = row[\"follower_list\"]\n",
    "    for follower in f_list:\n",
    "        graph.add_edge(follower, row[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab\n",
    "\n",
    "\n",
    "\n",
    "def save_graph(graph,file_name):\n",
    "    #initialze Figure\n",
    "    plt.figure(num=None, figsize=(150, 150), dpi=100)\n",
    "    plt.axis('off')\n",
    "    fig = plt.figure(1)\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw_networkx_nodes(graph,pos,node_color=range(len(graph)),cmap=plt.cm.Reds)\n",
    "    nx.draw_networkx_edges(graph,pos,alpha=0.4,arrows=False)\n",
    "\n",
    "    # cut = 1.00\n",
    "    # xmax = cut * max(xx for xx, yy in pos.values())\n",
    "    # ymax = cut * max(yy for xx, yy in pos.values())\n",
    "    # plt.xlim(0, xmax)\n",
    "    # plt.ylim(0, ymax)\n",
    "\n",
    "    plt.savefig(file_name)\n",
    "    pylab.close()\n",
    "    del fig\n",
    "\n",
    "#Assuming that the graph g has nodes and edges entered\n",
    "print(len(graph))\n",
    "save_graph(graph,\"my_graph_7.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d2b37dc4820977dd01f97017c0afe691699a3f985416c7f071ed6150a7a27c17"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
